{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100",
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "d020265098354ca09ee75939288cd60f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_4f1df81fb8e44a269959ad6050470e71",
              "IPY_MODEL_0102f2f4bf0c422892bca75eda4d5f9b",
              "IPY_MODEL_b175f4ad0dc54125a7048d73a4b2f296"
            ],
            "layout": "IPY_MODEL_4f55d83e2c54448a82f5710f8f263fad"
          }
        },
        "4f1df81fb8e44a269959ad6050470e71": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_48bcea2cd2d9409ca78e562e7727932a",
            "placeholder": "​",
            "style": "IPY_MODEL_a2aaebdf246a4f05b5dc1c6b52c11a48",
            "value": "tokenizer_config.json: 100%"
          }
        },
        "0102f2f4bf0c422892bca75eda4d5f9b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_da73133c3d7945ec9e1d732431701bad",
            "max": 1460,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_9a97e76872d1405ca72815477223c577",
            "value": 1460
          }
        },
        "b175f4ad0dc54125a7048d73a4b2f296": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e89fb7d2b2ec47ad8390692f0770554b",
            "placeholder": "​",
            "style": "IPY_MODEL_2fbee9f5335d44afa27ee3d9f344e569",
            "value": " 1.46k/1.46k [00:00&lt;00:00, 106kB/s]"
          }
        },
        "4f55d83e2c54448a82f5710f8f263fad": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "48bcea2cd2d9409ca78e562e7727932a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a2aaebdf246a4f05b5dc1c6b52c11a48": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "da73133c3d7945ec9e1d732431701bad": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9a97e76872d1405ca72815477223c577": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "e89fb7d2b2ec47ad8390692f0770554b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2fbee9f5335d44afa27ee3d9f344e569": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f33b909ccc4c42f7a1e3e5eeac600c71": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_f9f52155474941ca92f845b0434f93cd",
              "IPY_MODEL_2b1c39f3fab54ecda0adec190ad65f32",
              "IPY_MODEL_5673424ad0124af9908345bb36d143f2"
            ],
            "layout": "IPY_MODEL_c33d456662064be8be72b05112cf2647"
          }
        },
        "f9f52155474941ca92f845b0434f93cd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_36bd94d5a28d452a9ebc27ed6bbee6c9",
            "placeholder": "​",
            "style": "IPY_MODEL_fd85df9f9a824aacb07ac178ee61980e",
            "value": "tokenizer.model: 100%"
          }
        },
        "2b1c39f3fab54ecda0adec190ad65f32": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ef482343e9cd44e79f530b808b33ccd0",
            "max": 493443,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_ccf376b450d04301814f2bc6fd62495c",
            "value": 493443
          }
        },
        "5673424ad0124af9908345bb36d143f2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_504507adfa324c9cb7817fbd9794c997",
            "placeholder": "​",
            "style": "IPY_MODEL_dee63f55a5ac468c9c368c6fe0b74dfe",
            "value": " 493k/493k [00:00&lt;00:00, 8.61MB/s]"
          }
        },
        "c33d456662064be8be72b05112cf2647": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "36bd94d5a28d452a9ebc27ed6bbee6c9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fd85df9f9a824aacb07ac178ee61980e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ef482343e9cd44e79f530b808b33ccd0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ccf376b450d04301814f2bc6fd62495c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "504507adfa324c9cb7817fbd9794c997": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "dee63f55a5ac468c9c368c6fe0b74dfe": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ac7ec3817dcf4f8a8676f7d653f1da67": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_7700c48e9fd0468dbdda9e260d88d093",
              "IPY_MODEL_db0b424831224ce58be5c465ae9bdba2",
              "IPY_MODEL_92edf25a37f74ab2a62c80ef21951df7"
            ],
            "layout": "IPY_MODEL_25ce6c7a58174438a5e54758e6dda7b7"
          }
        },
        "7700c48e9fd0468dbdda9e260d88d093": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_75c853e8fc3942ad95411540616aba70",
            "placeholder": "​",
            "style": "IPY_MODEL_86fe97474733444f864e01552ded08d1",
            "value": "tokenizer.json: 100%"
          }
        },
        "db0b424831224ce58be5c465ae9bdba2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_05b797a215d94489a65647c202d01c26",
            "max": 1795303,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_2dc8e41d73a44c26aa6cc612fef66f7f",
            "value": 1795303
          }
        },
        "92edf25a37f74ab2a62c80ef21951df7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9753a070e8044ee5bc4a72aee4ec8eaf",
            "placeholder": "​",
            "style": "IPY_MODEL_cdd0982b20204ad185ea015fad2febb7",
            "value": " 1.80M/1.80M [00:00&lt;00:00, 19.1MB/s]"
          }
        },
        "25ce6c7a58174438a5e54758e6dda7b7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "75c853e8fc3942ad95411540616aba70": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "86fe97474733444f864e01552ded08d1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "05b797a215d94489a65647c202d01c26": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2dc8e41d73a44c26aa6cc612fef66f7f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "9753a070e8044ee5bc4a72aee4ec8eaf": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cdd0982b20204ad185ea015fad2febb7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7319c640b35a47ba895591a1cd861e65": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_f8cbaf0e64454d82a33328a9fe5b3f90",
              "IPY_MODEL_d95fb19f08b14b6eaf61334de29c0717",
              "IPY_MODEL_d744193672dd43b0ba076238c52ac57c"
            ],
            "layout": "IPY_MODEL_30cb4ee5b40a46d0b346c123e6feba70"
          }
        },
        "f8cbaf0e64454d82a33328a9fe5b3f90": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_997b305f1b5b4c3296e291a9b858bc91",
            "placeholder": "​",
            "style": "IPY_MODEL_ab7a9b36457e4486913263307af18233",
            "value": "special_tokens_map.json: 100%"
          }
        },
        "d95fb19f08b14b6eaf61334de29c0717": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7b8fffb2868840229688e9c7f1e20f34",
            "max": 72,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_9c61a856c8054d63ad3fea40e6854dfd",
            "value": 72
          }
        },
        "d744193672dd43b0ba076238c52ac57c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_053ec07e9b6e48d68733ef0155ddb325",
            "placeholder": "​",
            "style": "IPY_MODEL_9a313141ba694b80b46d22d1b3797b33",
            "value": " 72.0/72.0 [00:00&lt;00:00, 6.91kB/s]"
          }
        },
        "30cb4ee5b40a46d0b346c123e6feba70": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "997b305f1b5b4c3296e291a9b858bc91": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ab7a9b36457e4486913263307af18233": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7b8fffb2868840229688e9c7f1e20f34": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9c61a856c8054d63ad3fea40e6854dfd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "053ec07e9b6e48d68733ef0155ddb325": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9a313141ba694b80b46d22d1b3797b33": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Mixtral in Colab\n",
        "\n",
        "Welcome! In this notebook you can run [Mixtral8x7B-Instruct](https://huggingface.co/mistralai/Mixtral-8x7B-v0.1) with decent generation speed **right in Google Colab or on a consumer-grade GPU**. This was made possible by quantizing the original model in mixed precision and implementing a MoE-specific offloading strategy. This build was also included with the NVIDIA LLM-RT code so the graphics card can optimize this model's runtime to be even faster in addition to the above features. Note: Quantization does reduce a small amount of the models accuracy in favor of speed and preformance but the purpose of this model is to run as quickly as possible with the least amount of preformance loss. An additional note is that this code was intended to run on an A100 NVIDA graphics card and may have unforseen bugs or other problems when run on another kind of processing card.\n",
        "\n",
        "For the original unmodified code, please see the below resources. I am not at all affiliated with the teams below but I used their open source resources for the Mixtral optimization while I added some code changes to it, most of the code is from the original sources below: with the exception of the NVIDIA LLM-RT codes of course!\n",
        "To learn more, read Deniz Azur's [tech report](https://arxiv.org/abs/2312.17238) or check out the [repo](https://github.com/dvmazur/mixtral-offloading) on GitHub.\n",
        "\n",
        "The LLM-RT NVIDIA code was altered to fit on a cloud service such as Google Collab where I created this code. This is the original altered code and the base engine creation credit goes to the NVIDIA team and their repository here: https://github.com/NVIDIA/TensorRT-LLM\n",
        "\n",
        "My own code built for Google Collab (if you wanted to test the code yourself) is located here: https://github.com/viasky657/GoogleCollabFiles"
      ],
      "metadata": {
        "id": "OW1moHJ1TdhO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "!zip -r logMixtral-8x7B-Instruct-v0.1-offloading-demo.zip/ Mixtral-8x7B-Instruct-v0.1-offloading-demo\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ni7Ii6Tm0Kxq",
        "outputId": "3b1ce1b9-7750-42bf-dd18-4320c90eeb88"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\tzip warning: name not matched: Mixtral-8x7B-Instruct-v0.1-offloading-demo\n",
            "\n",
            "zip error: Nothing to do! (try: zip -r logMixtral-8x7B-Instruct-v0.1-offloading-demo.zip/ . -i Mixtral-8x7B-Instruct-v0.1-offloading-demo)\n",
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "One will need approximately 16 GB of VRAM and 11 GB of RAM to run this notebook and generate somewhat long texts.\n",
        "\n",
        "\n",
        "<details>\n",
        "\n",
        "<summary>How to balance between RAM and GPU VRAM usage</summary>\n",
        "\n",
        "You can balance between RAM and GPU VRAM usage by changing <code>offload_per_layer</code> variable in the <a href=\"#scrollTo=_mIpePTMFyRY&line=10&uniqifier=1\">Initialize model</a> section. Increasing <code>offload_per_layer</code> will decrease GPU VRAM usage, increase RAM usage and decrease generation speed. Decreasing <code>offload_per_layer</code> will have the opposite effect.\n",
        "\n",
        "Note that this notebook should run normally in Google Colab with <code>offload_per_layer = 4</code>, but may crush with other values. However, if you run this somewhere else, you're free to play with this variable.\n",
        "</details>"
      ],
      "metadata": {
        "id": "2-dvAX_hKZT4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# LLM - RT Set - Up\n",
        "**LLM-RT Tensor Set-Up for Optimized GPU Speed Preformance**\n",
        "\n",
        "Step 1: Install the NVIDIA Container Toolkit from Here - https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/latest/install-guide.html\n",
        "\n"
      ],
      "metadata": {
        "id": "c57B-lA5YjuD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "!curl -fsSL https://get.docker.com -o get-docker.sh\n",
        "!sh get-docker.sh\n",
        "\n",
        "!distribution=$(. /etc/os-release;echo $ID$VERSION_ID) \\\n",
        "   && curl -s -L https://nvidia.github.io/nvidia-docker/gpgkey | sudo apt-key add - \\\n",
        "   && curl -s -L https://nvidia.github.io/nvidia-docker/$distribution/nvidia-docker.list | sudo tee /etc/apt/sources.list.d/nvidia-docker.list\n",
        "!sudo apt-get update\n",
        "!sudo apt-get install -y nvidia-docker2\n",
        "!sudo systemctl restart docker\n",
        "\n",
        "!docker run --gpus all llm-rt-image"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GKJzKt5HaGPi",
        "outputId": "a5c16f18-22ee-4c94-8375-7a087ebda479"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "# Executing docker install script, commit: e5543d473431b782227f8908005543bb4389b8de\n",
            "+ sh -c apt-get update -qq >/dev/null\n",
            "+ sh -c DEBIAN_FRONTEND=noninteractive apt-get install -y -qq apt-transport-https ca-certificates curl >/dev/null\n",
            "+ sh -c install -m 0755 -d /etc/apt/keyrings\n",
            "+ sh -c curl -fsSL \"https://download.docker.com/linux/ubuntu/gpg\" | gpg --dearmor --yes -o /etc/apt/keyrings/docker.gpg\n",
            "+ sh -c chmod a+r /etc/apt/keyrings/docker.gpg\n",
            "+ sh -c echo \"deb [arch=amd64 signed-by=/etc/apt/keyrings/docker.gpg] https://download.docker.com/linux/ubuntu jammy stable\" > /etc/apt/sources.list.d/docker.list\n",
            "+ sh -c apt-get update -qq >/dev/null\n",
            "+ sh -c DEBIAN_FRONTEND=noninteractive apt-get install -y -qq docker-ce docker-ce-cli containerd.io docker-compose-plugin docker-ce-rootless-extras docker-buildx-plugin >/dev/null\n",
            "\n",
            "================================================================================\n",
            "\n",
            "To run Docker as a non-privileged user, consider setting up the\n",
            "Docker daemon in rootless mode for your user:\n",
            "\n",
            "    dockerd-rootless-setuptool.sh install\n",
            "\n",
            "Visit https://docs.docker.com/go/rootless/ to learn about rootless mode.\n",
            "\n",
            "\n",
            "To run the Docker daemon as a fully privileged service, but granting non-root\n",
            "users access, refer to https://docs.docker.com/go/daemon-access/\n",
            "\n",
            "WARNING: Access to the remote API on a privileged Docker daemon is equivalent\n",
            "         to root access on the host. Refer to the 'Docker daemon attack surface'\n",
            "         documentation for details: https://docs.docker.com/go/attack-surface/\n",
            "\n",
            "================================================================================\n",
            "\n",
            "Warning: apt-key is deprecated. Manage keyring files in trusted.gpg.d instead (see apt-key(8)).\n",
            "OK\n",
            "deb https://nvidia.github.io/libnvidia-container/stable/ubuntu18.04/$(ARCH) /\n",
            "#deb https://nvidia.github.io/libnvidia-container/experimental/ubuntu18.04/$(ARCH) /\n",
            "deb https://nvidia.github.io/nvidia-container-runtime/stable/ubuntu18.04/$(ARCH) /\n",
            "#deb https://nvidia.github.io/nvidia-container-runtime/experimental/ubuntu18.04/$(ARCH) /\n",
            "deb https://nvidia.github.io/nvidia-docker/ubuntu18.04/$(ARCH) /\n",
            "Hit:1 https://download.docker.com/linux/ubuntu jammy InRelease\n",
            "Get:2 https://nvidia.github.io/libnvidia-container/stable/ubuntu18.04/amd64  InRelease [1,484 B]\n",
            "Get:3 https://nvidia.github.io/nvidia-container-runtime/stable/ubuntu18.04/amd64  InRelease [1,481 B]\n",
            "Get:4 https://nvidia.github.io/nvidia-docker/ubuntu18.04/amd64  InRelease [1,474 B]\n",
            "Hit:5 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease\n",
            "Get:6 https://nvidia.github.io/libnvidia-container/stable/ubuntu18.04/amd64  Packages [29.2 kB]\n",
            "Hit:7 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease\n",
            "Get:8 https://nvidia.github.io/nvidia-container-runtime/stable/ubuntu18.04/amd64  Packages [7,416 B]\n",
            "Hit:9 http://security.ubuntu.com/ubuntu jammy-security InRelease\n",
            "Hit:10 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Get:11 https://nvidia.github.io/nvidia-docker/ubuntu18.04/amd64  Packages [4,488 B]\n",
            "Hit:12 http://archive.ubuntu.com/ubuntu jammy-updates InRelease\n",
            "Hit:13 https://ppa.launchpadcontent.net/c2d4u.team/c2d4u4.0+/ubuntu jammy InRelease\n",
            "Hit:14 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Hit:15 http://archive.ubuntu.com/ubuntu jammy-backports InRelease\n",
            "Hit:16 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Hit:17 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Fetched 45.6 kB in 1s (32.8 kB/s)\n",
            "Reading package lists... Done\n",
            "W: https://nvidia.github.io/libnvidia-container/stable/ubuntu18.04/amd64/InRelease: Key is stored in legacy trusted.gpg keyring (/etc/apt/trusted.gpg), see the DEPRECATION section in apt-key(8) for details.\n",
            "W: https://nvidia.github.io/nvidia-container-runtime/stable/ubuntu18.04/amd64/InRelease: Key is stored in legacy trusted.gpg keyring (/etc/apt/trusted.gpg), see the DEPRECATION section in apt-key(8) for details.\n",
            "W: https://nvidia.github.io/nvidia-docker/ubuntu18.04/amd64/InRelease: Key is stored in legacy trusted.gpg keyring (/etc/apt/trusted.gpg), see the DEPRECATION section in apt-key(8) for details.\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "The following additional packages will be installed:\n",
            "  libnvidia-container-tools libnvidia-container1 nvidia-container-toolkit\n",
            "  nvidia-container-toolkit-base\n",
            "The following NEW packages will be installed:\n",
            "  libnvidia-container-tools libnvidia-container1 nvidia-container-toolkit\n",
            "  nvidia-container-toolkit-base nvidia-docker2\n",
            "0 upgraded, 5 newly installed, 0 to remove and 39 not upgraded.\n",
            "Need to get 4,211 kB of archives.\n",
            "After this operation, 16.7 MB of additional disk space will be used.\n",
            "Get:1 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  libnvidia-container1 1.14.5-1 [923 kB]\n",
            "Get:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  libnvidia-container-tools 1.14.5-1 [20.6 kB]\n",
            "Get:3 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  nvidia-container-toolkit-base 1.14.5-1 [2,339 kB]\n",
            "Get:4 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  nvidia-container-toolkit 1.14.5-1 [921 kB]\n",
            "Get:5 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  nvidia-docker2 2.13.0-1 [6,876 B]\n",
            "Fetched 4,211 kB in 3s (1,383 kB/s)\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 78, <> line 5.)\n",
            "debconf: falling back to frontend: Readline\n",
            "debconf: unable to initialize frontend: Readline\n",
            "debconf: (This frontend requires a controlling tty.)\n",
            "debconf: falling back to frontend: Teletype\n",
            "dpkg-preconfigure: unable to re-open stdin: \n",
            "Selecting previously unselected package libnvidia-container1:amd64.\n",
            "(Reading database ... 123345 files and directories currently installed.)\n",
            "Preparing to unpack .../libnvidia-container1_1.14.5-1_amd64.deb ...\n",
            "Unpacking libnvidia-container1:amd64 (1.14.5-1) ...\n",
            "Selecting previously unselected package libnvidia-container-tools.\n",
            "Preparing to unpack .../libnvidia-container-tools_1.14.5-1_amd64.deb ...\n",
            "Unpacking libnvidia-container-tools (1.14.5-1) ...\n",
            "Selecting previously unselected package nvidia-container-toolkit-base.\n",
            "Preparing to unpack .../nvidia-container-toolkit-base_1.14.5-1_amd64.deb ...\n",
            "Unpacking nvidia-container-toolkit-base (1.14.5-1) ...\n",
            "Selecting previously unselected package nvidia-container-toolkit.\n",
            "Preparing to unpack .../nvidia-container-toolkit_1.14.5-1_amd64.deb ...\n",
            "Unpacking nvidia-container-toolkit (1.14.5-1) ...\n",
            "Selecting previously unselected package nvidia-docker2.\n",
            "Preparing to unpack .../nvidia-docker2_2.13.0-1_all.deb ...\n",
            "Unpacking nvidia-docker2 (2.13.0-1) ...\n",
            "Setting up nvidia-container-toolkit-base (1.14.5-1) ...\n",
            "Setting up libnvidia-container1:amd64 (1.14.5-1) ...\n",
            "Setting up libnvidia-container-tools (1.14.5-1) ...\n",
            "Setting up nvidia-container-toolkit (1.14.5-1) ...\n",
            "Setting up nvidia-docker2 (2.13.0-1) ...\n",
            "Processing triggers for libc-bin (2.35-0ubuntu3.4) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbb.so.12 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_5.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_0.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc_proxy.so.2 is not a symbolic link\n",
            "\n",
            "System has not been booted with systemd as init system (PID 1). Can't operate.\n",
            "Failed to connect to bus: Host is down\n",
            "docker: Cannot connect to the Docker daemon at unix:///var/run/docker.sock. Is the docker daemon running?.\n",
            "See 'docker run --help'.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Next Step:  Install TensorRT-LLM for x86_64 users for Mixtral AI and Install for Distilled Whisper Build AI.**"
      ],
      "metadata": {
        "id": "KHuHGfWeEmeF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "#MIXTRAL AI Install\n",
        "# Install dependencies\n",
        "!apt-get update\n",
        "!apt-get -y install python3.10 python3-pip openmpi-bin libopenmpi-dev\n",
        "\n",
        "# Install the latest preview version (corresponding to the main branch) of TensorRT-LLM.\n",
        "# If you want to install the stable version (corresponding to the release branch), please\n",
        "# remove the `--pre` option.\n",
        "!pip3 install tensorrt_llm -U --extra-index-url https://pypi.nvidia.com\n",
        "\n",
        "# Check installation\n",
        "!python3 -c \"import tensorrt_llm\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "coxYvcULEkqH",
        "outputId": "574b8a0f-77b8-42f6-d62c-b091e76cbfda"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r0% [Working]\r            \rGet:1 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease [3,626 B]\n",
            "Get:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease [1,581 B]\n",
            "Get:3 http://security.ubuntu.com/ubuntu jammy-security InRelease [110 kB]\n",
            "Hit:4 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Get:5 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [119 kB]\n",
            "Hit:6 https://ppa.launchpadcontent.net/c2d4u.team/c2d4u4.0+/ubuntu jammy InRelease\n",
            "Hit:7 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Hit:8 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Hit:9 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Hit:10 http://archive.ubuntu.com/ubuntu jammy-backports InRelease\n",
            "Get:11 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 Packages [1,786 kB]\n",
            "Get:12 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  Packages [713 kB]\n",
            "Get:13 http://security.ubuntu.com/ubuntu jammy-security/main amd64 Packages [1,502 kB]\n",
            "Get:14 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 Packages [1,343 kB]\n",
            "Get:15 http://archive.ubuntu.com/ubuntu jammy-updates/restricted amd64 Packages [1,907 kB]\n",
            "Get:16 http://security.ubuntu.com/ubuntu jammy-security/restricted amd64 Packages [1,859 kB]\n",
            "Fetched 9,345 kB in 6s (1,468 kB/s)\n",
            "Reading package lists... Done\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "libopenmpi-dev is already the newest version (4.1.2-2ubuntu1).\n",
            "openmpi-bin is already the newest version (4.1.2-2ubuntu1).\n",
            "openmpi-bin set to manually installed.\n",
            "python3.10 is already the newest version (3.10.12-1~22.04.3).\n",
            "The following additional packages will be installed:\n",
            "  python3-setuptools python3-wheel\n",
            "Suggested packages:\n",
            "  python-setuptools-doc\n",
            "The following NEW packages will be installed:\n",
            "  python3-pip python3-setuptools python3-wheel\n",
            "0 upgraded, 3 newly installed, 0 to remove and 39 not upgraded.\n",
            "Need to get 1,677 kB of archives.\n",
            "After this operation, 8,967 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 python3-setuptools all 59.6.0-1.2ubuntu0.22.04.1 [339 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 python3-wheel all 0.37.1-2ubuntu0.22.04.1 [32.0 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 python3-pip all 22.0.2+dfsg-1ubuntu0.4 [1,305 kB]\n",
            "Fetched 1,677 kB in 2s (885 kB/s)\n",
            "Selecting previously unselected package python3-setuptools.\n",
            "(Reading database ... 121749 files and directories currently installed.)\n",
            "Preparing to unpack .../python3-setuptools_59.6.0-1.2ubuntu0.22.04.1_all.deb ...\n",
            "Unpacking python3-setuptools (59.6.0-1.2ubuntu0.22.04.1) ...\n",
            "Selecting previously unselected package python3-wheel.\n",
            "Preparing to unpack .../python3-wheel_0.37.1-2ubuntu0.22.04.1_all.deb ...\n",
            "Unpacking python3-wheel (0.37.1-2ubuntu0.22.04.1) ...\n",
            "Selecting previously unselected package python3-pip.\n",
            "Preparing to unpack .../python3-pip_22.0.2+dfsg-1ubuntu0.4_all.deb ...\n",
            "Unpacking python3-pip (22.0.2+dfsg-1ubuntu0.4) ...\n",
            "Setting up python3-setuptools (59.6.0-1.2ubuntu0.22.04.1) ...\n",
            "Setting up python3-wheel (0.37.1-2ubuntu0.22.04.1) ...\n",
            "Setting up python3-pip (22.0.2+dfsg-1ubuntu0.4) ...\n",
            "Processing triggers for man-db (2.10.2-1) ...\n",
            "Looking in indexes: https://pypi.org/simple, https://pypi.nvidia.com\n",
            "Collecting tensorrt_llm\n",
            "  Downloading https://pypi.nvidia.com/tensorrt-llm/tensorrt_llm-0.7.1-cp310-cp310-linux_x86_64.whl (340.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m340.3/340.3 MB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting accelerate==0.20.3 (from tensorrt_llm)\n",
            "  Downloading accelerate-0.20.3-py3-none-any.whl (227 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m227.6/227.6 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: build in /usr/local/lib/python3.10/dist-packages (from tensorrt_llm) (1.0.3)\n",
            "Collecting colored (from tensorrt_llm)\n",
            "  Downloading colored-2.2.4-py3-none-any.whl (16 kB)\n",
            "Collecting cuda-python==12.2.0 (from tensorrt_llm)\n",
            "  Downloading cuda_python-12.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (20.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m20.1/20.1 MB\u001b[0m \u001b[31m11.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting diffusers==0.15.0 (from tensorrt_llm)\n",
            "  Downloading diffusers-0.15.0-py3-none-any.whl (851 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m851.8/851.8 kB\u001b[0m \u001b[31m60.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting lark (from tensorrt_llm)\n",
            "  Downloading lark-1.1.9-py3-none-any.whl (111 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m111.7/111.7 kB\u001b[0m \u001b[31m15.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting mpi4py (from tensorrt_llm)\n",
            "  Downloading mpi4py-3.1.5.tar.gz (2.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from tensorrt_llm) (1.25.2)\n",
            "Collecting onnx>=1.12.0 (from tensorrt_llm)\n",
            "  Downloading onnx-1.15.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (15.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15.7/15.7 MB\u001b[0m \u001b[31m84.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting polygraphy (from tensorrt_llm)\n",
            "  Downloading https://pypi.nvidia.com/polygraphy/polygraphy-0.49.0-py2.py3-none-any.whl (327 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m327.9/327.9 kB\u001b[0m \u001b[31m32.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: sentencepiece>=0.1.99 in /usr/local/lib/python3.10/dist-packages (from tensorrt_llm) (0.1.99)\n",
            "Collecting tensorrt==9.2.0.post12.dev5 (from tensorrt_llm)\n",
            "  Downloading https://pypi.nvidia.com/tensorrt/tensorrt-9.2.0.post12.dev5.tar.gz (18 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from tensorrt_llm) (2.1.0+cu121)\n",
            "Collecting transformers==4.33.1 (from tensorrt_llm)\n",
            "  Downloading transformers-4.33.1-py3-none-any.whl (7.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.6/7.6 MB\u001b[0m \u001b[31m94.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: wheel in /usr/local/lib/python3.10/dist-packages (from tensorrt_llm) (0.42.0)\n",
            "Collecting optimum (from tensorrt_llm)\n",
            "  Downloading optimum-1.17.1-py3-none-any.whl (407 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m407.1/407.1 kB\u001b[0m \u001b[31m38.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting evaluate (from tensorrt_llm)\n",
            "  Downloading evaluate-0.4.1-py3-none-any.whl (84 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.1/84.1 kB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate==0.20.3->tensorrt_llm) (23.2)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate==0.20.3->tensorrt_llm) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate==0.20.3->tensorrt_llm) (6.0.1)\n",
            "Requirement already satisfied: cython in /usr/local/lib/python3.10/dist-packages (from cuda-python==12.2.0->tensorrt_llm) (3.0.8)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from diffusers==0.15.0->tensorrt_llm) (9.4.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from diffusers==0.15.0->tensorrt_llm) (3.13.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.13.2 in /usr/local/lib/python3.10/dist-packages (from diffusers==0.15.0->tensorrt_llm) (0.20.3)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.10/dist-packages (from diffusers==0.15.0->tensorrt_llm) (7.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from diffusers==0.15.0->tensorrt_llm) (2023.12.25)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from diffusers==0.15.0->tensorrt_llm) (2.31.0)\n",
            "Collecting tensorrt_libs==9.2.0.post12.dev5 (from tensorrt==9.2.0.post12.dev5->tensorrt_llm)\n",
            "  Downloading https://pypi.nvidia.com/tensorrt-libs/tensorrt_libs-9.2.0.post12.dev5-py2.py3-none-manylinux_2_17_x86_64.whl (1076.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 GB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tensorrt_bindings==9.2.0.post12.dev5 (from tensorrt==9.2.0.post12.dev5->tensorrt_llm)\n",
            "  Downloading https://pypi.nvidia.com/tensorrt-bindings/tensorrt_bindings-9.2.0.post12.dev5-cp310-none-manylinux_2_17_x86_64.whl (1.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m56.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers==4.33.1->tensorrt_llm)\n",
            "  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m110.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.33.1->tensorrt_llm) (0.4.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers==4.33.1->tensorrt_llm) (4.66.2)\n",
            "Collecting nvidia-cuda-runtime-cu12 (from tensorrt_libs==9.2.0.post12.dev5->tensorrt==9.2.0.post12.dev5->tensorrt_llm)\n",
            "  Downloading https://pypi.nvidia.com/nvidia-cuda-runtime-cu12/nvidia_cuda_runtime_cu12-12.3.101-py3-none-manylinux1_x86_64.whl (867 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m867.7/867.7 kB\u001b[0m \u001b[31m52.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cudnn-cu12 (from tensorrt_libs==9.2.0.post12.dev5->tensorrt==9.2.0.post12.dev5->tensorrt_llm)\n",
            "  Downloading https://pypi.nvidia.com/nvidia-cudnn-cu12/nvidia_cudnn_cu12-8.9.7.29-py3-none-manylinux1_x86_64.whl (704.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m704.7/704.7 MB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cublas-cu12 (from tensorrt_libs==9.2.0.post12.dev5->tensorrt==9.2.0.post12.dev5->tensorrt_llm)\n",
            "  Downloading https://pypi.nvidia.com/nvidia-cublas-cu12/nvidia_cublas_cu12-12.3.4.1-py3-none-manylinux1_x86_64.whl (412.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m412.6/412.6 MB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: protobuf>=3.20.2 in /usr/local/lib/python3.10/dist-packages (from onnx>=1.12.0->tensorrt_llm) (3.20.3)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch->tensorrt_llm) (4.9.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->tensorrt_llm) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->tensorrt_llm) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->tensorrt_llm) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->tensorrt_llm) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch->tensorrt_llm) (2.1.0)\n",
            "Requirement already satisfied: pyproject_hooks in /usr/local/lib/python3.10/dist-packages (from build->tensorrt_llm) (1.0.0)\n",
            "Requirement already satisfied: tomli>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from build->tensorrt_llm) (2.0.1)\n",
            "Collecting datasets>=2.0.0 (from evaluate->tensorrt_llm)\n",
            "  Downloading datasets-2.17.1-py3-none-any.whl (536 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m536.7/536.7 kB\u001b[0m \u001b[31m43.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting dill (from evaluate->tensorrt_llm)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m16.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from evaluate->tensorrt_llm) (1.5.3)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from evaluate->tensorrt_llm) (3.4.1)\n",
            "Collecting multiprocess (from evaluate->tensorrt_llm)\n",
            "  Downloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m17.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting responses<0.19 (from evaluate->tensorrt_llm)\n",
            "  Downloading responses-0.18.0-py3-none-any.whl (38 kB)\n",
            "Collecting coloredlogs (from optimum->tensorrt_llm)\n",
            "  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m56.2 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: transformers[sentencepiece]>=4.26.0 in /usr/local/lib/python3.10/dist-packages (from optimum->tensorrt_llm) (4.37.2)\n",
            "Requirement already satisfied: pyarrow>=12.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate->tensorrt_llm) (14.0.2)\n",
            "Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate->tensorrt_llm) (0.6)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate->tensorrt_llm) (3.9.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->diffusers==0.15.0->tensorrt_llm) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->diffusers==0.15.0->tensorrt_llm) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->diffusers==0.15.0->tensorrt_llm) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->diffusers==0.15.0->tensorrt_llm) (2024.2.2)\n",
            "INFO: pip is looking at multiple versions of transformers[sentencepiece] to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting transformers[sentencepiece]>=4.26.0 (from optimum->tensorrt_llm)\n",
            "  Downloading transformers-4.38.1-py3-none-any.whl (8.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.5/8.5 MB\u001b[0m \u001b[31m116.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading transformers-4.38.0-py3-none-any.whl (8.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.5/8.5 MB\u001b[0m \u001b[31m91.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading transformers-4.37.1-py3-none-any.whl (8.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.4/8.4 MB\u001b[0m \u001b[31m90.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading transformers-4.37.0-py3-none-any.whl (8.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.4/8.4 MB\u001b[0m \u001b[31m114.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading transformers-4.36.2-py3-none-any.whl (8.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.2/8.2 MB\u001b[0m \u001b[31m106.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Using cached transformers-4.36.1-py3-none-any.whl (8.3 MB)\n",
            "  Downloading transformers-4.36.0-py3-none-any.whl (8.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.2/8.2 MB\u001b[0m \u001b[31m94.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hINFO: pip is looking at multiple versions of transformers[sentencepiece] to determine which version is compatible with other requirements. This could take a while.\n",
            "  Downloading transformers-4.35.2-py3-none-any.whl (7.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.9/7.9 MB\u001b[0m \u001b[31m98.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading transformers-4.35.1-py3-none-any.whl (7.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.9/7.9 MB\u001b[0m \u001b[31m86.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading transformers-4.35.0-py3-none-any.whl (7.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.9/7.9 MB\u001b[0m \u001b[31m58.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading transformers-4.34.1-py3-none-any.whl (7.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.7/7.7 MB\u001b[0m \u001b[31m93.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading transformers-4.34.0-py3-none-any.whl (7.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.7/7.7 MB\u001b[0m \u001b[31m60.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hINFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. See https://pip.pypa.io/warnings/backtracking for guidance. If you want to abort this run, press Ctrl + C.\n",
            "  Downloading transformers-4.33.3-py3-none-any.whl (7.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.6/7.6 MB\u001b[0m \u001b[31m98.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading transformers-4.33.2-py3-none-any.whl (7.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.6/7.6 MB\u001b[0m \u001b[31m98.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting humanfriendly>=9.1 (from coloredlogs->optimum->tensorrt_llm)\n",
            "  Downloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.10/dist-packages (from importlib-metadata->diffusers==0.15.0->tensorrt_llm) (3.17.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->tensorrt_llm) (2.1.5)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->evaluate->tensorrt_llm) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->evaluate->tensorrt_llm) (2023.4)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->tensorrt_llm) (1.3.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate->tensorrt_llm) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate->tensorrt_llm) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate->tensorrt_llm) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate->tensorrt_llm) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate->tensorrt_llm) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate->tensorrt_llm) (4.0.3)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->evaluate->tensorrt_llm) (1.16.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12 (from nvidia-cudnn-cu12->tensorrt_libs==9.2.0.post12.dev5->tensorrt==9.2.0.post12.dev5->tensorrt_llm)\n",
            "  Downloading https://pypi.nvidia.com/nvidia-cuda-nvrtc-cu12/nvidia_cuda_nvrtc_cu12-12.3.107-py3-none-manylinux1_x86_64.whl (24.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.9/24.9 MB\u001b[0m \u001b[31m52.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: tensorrt, mpi4py\n",
            "  Building wheel for tensorrt (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for tensorrt: filename=tensorrt-9.2.0.post12.dev5-py2.py3-none-any.whl size=17625 sha256=b45a46814ff4afd26137b996a68ca802f5d9b82806587eda00303a53f763b7ba\n",
            "  Stored in directory: /root/.cache/pip/wheels/aa/96/bf/028c219d3560856a5fdb8b3aec8bf01e9d485521c092a64d02\n",
            "  Building wheel for mpi4py (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for mpi4py: filename=mpi4py-3.1.5-cp310-cp310-linux_x86_64.whl size=2746506 sha256=c6f06edb13958dabe4be867a6f93fc3d8be08b946ea8e91108d72122c34402e8\n",
            "  Stored in directory: /root/.cache/pip/wheels/18/2b/7f/c852523089e9182b45fca50ff56f49a51eeb6284fd25a66713\n",
            "Successfully built tensorrt mpi4py\n",
            "Installing collected packages: tokenizers, tensorrt_bindings, polygraphy, onnx, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cublas-cu12, mpi4py, lark, humanfriendly, dill, cuda-python, colored, responses, nvidia-cudnn-cu12, multiprocess, coloredlogs, transformers, tensorrt_libs, diffusers, accelerate, tensorrt, datasets, optimum, evaluate, tensorrt_llm\n",
            "  Attempting uninstall: tokenizers\n",
            "    Found existing installation: tokenizers 0.15.2\n",
            "    Uninstalling tokenizers-0.15.2:\n",
            "      Successfully uninstalled tokenizers-0.15.2\n",
            "  Attempting uninstall: transformers\n",
            "    Found existing installation: transformers 4.37.2\n",
            "    Uninstalling transformers-4.37.2:\n",
            "      Successfully uninstalled transformers-4.37.2\n",
            "Successfully installed accelerate-0.20.3 colored-2.2.4 coloredlogs-15.0.1 cuda-python-12.2.0 datasets-2.17.1 diffusers-0.15.0 dill-0.3.8 evaluate-0.4.1 humanfriendly-10.0 lark-1.1.9 mpi4py-3.1.5 multiprocess-0.70.16 nvidia-cublas-cu12-12.3.4.1 nvidia-cuda-nvrtc-cu12-12.3.107 nvidia-cuda-runtime-cu12-12.3.101 nvidia-cudnn-cu12-8.9.7.29 onnx-1.15.0 optimum-1.17.1 polygraphy-0.49.0 responses-0.18.0 tensorrt-9.2.0.post12.dev5 tensorrt_bindings-9.2.0.post12.dev5 tensorrt_libs-9.2.0.post12.dev5 tensorrt_llm-0.7.1 tokenizers-0.13.3 transformers-4.33.1\n",
            "[02/24/2024-05:15:42] [TRT-LLM] [W] A required package 'psutil' is not installed. Will not monitor the device memory usages. Please install the package first, e.g, 'pip install pynvml>=11.5.0'.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install pynvml>=11.5.0 #Needed to monitor system memory for TensorRT-LLM."
      ],
      "metadata": {
        "id": "8uMS4zb3Vv8F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "****"
      ],
      "metadata": {
        "id": "28Ii3f4gJD9t"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Install and import libraries for Mixtral AI and for All AI Files"
      ],
      "metadata": {
        "id": "Y8MhvkC7TKEL"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f7qY7ebqX7T7"
      },
      "outputs": [],
      "source": [
        "# fix numpy in colab\n",
        "import numpy\n",
        "from IPython.display import clear_output\n",
        "!pip install PyAudio\n",
        "# fix triton in colab\n",
        "!export LC_ALL=\"en_US.UTF-8\"\n",
        "!export LD_LIBRARY_PATH=\"/usr/lib64-nvidia\"\n",
        "!export LIBRARY_PATH=\"/usr/local/cuda/lib64/stubs\"\n",
        "!ldconfig /usr/lib64-nvidia\n",
        "\n",
        "!git clone https://github.com/viasky657/GoogleCollabFiles.git                      #https://github.com/dvmazur/mixtral-offloading.git\n",
        "!cd GoogleCollabFiles/mixtral-offloading && pip install -q -r requirements.txt                                               #mixtral-offloading\n",
        "!huggingface-cli download lavawolfiee/Mixtral-8x7B-Instruct-v0.1-offloading-demo --quiet --local-dir Mixtral-8x7B-Instruct-v0.1-offloading-demo\n",
        "\n",
        "clear_output()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "\n",
        "sys.path.append(\"GoogleCollabFiles/mixtral-offloading\")\n",
        "import torch\n",
        "from torch.nn import functional as F\n",
        "from hqq.core.quantize import BaseQuantizeConfig\n",
        "from huggingface_hub import snapshot_download\n",
        "from IPython.display import clear_output\n",
        "from tqdm.auto import trange\n",
        "from transformers import AutoConfig, AutoTokenizer\n",
        "from transformers.utils import logging as hf_logging\n",
        "\n",
        "sys.path.append(\"GoogleCollabFiles/mixtral-offloading/src\")\n",
        "from src.build_model import OffloadConfig, QuantConfig, build_model                        #src.build_model"
      ],
      "metadata": {
        "id": "GgpjnV7fV49W",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 384
        },
        "outputId": "96c0a60e-2e57-4eb6-a456-d8d988251f98"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'hqq'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-1f5e602932df>\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfunctional\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mhqq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mquantize\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBaseQuantizeConfig\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mhuggingface_hub\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msnapshot_download\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mIPython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdisplay\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mclear_output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'hqq'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Initialize model"
      ],
      "metadata": {
        "id": "OkSYibHcTQsH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = \"mistralai/Mixtral-8x7B-Instruct-v0.1\"\n",
        "quantized_model_name = \"lavawolfiee/Mixtral-8x7B-Instruct-v0.1-offloading-demo\"\n",
        "state_path = \"Mixtral-8x7B-Instruct-v0.1-offloading-demo\"\n",
        "!git clone https://github.com/NVIDIA/TensorRT-LLM/tree/0ab9d17a59c284d2de36889832fe9fc7c8697604/tensorrt_llm\n",
        "\n",
        "config = AutoConfig.from_pretrained(quantized_model_name)\n",
        "\n",
        "device = torch.device(\"cuda:0\")\n",
        "\n",
        "##### Change this to 5 if you have only 12 GB of GPU VRAM #####\n",
        "offload_per_layer = 4\n",
        "# offload_per_layer = 5\n",
        "###############################################################\n",
        "\n",
        "num_experts = config.num_local_experts\n",
        "\n",
        "offload_config = OffloadConfig(\n",
        "    main_size=config.num_hidden_layers * (num_experts - offload_per_layer),\n",
        "    offload_size=config.num_hidden_layers * offload_per_layer,\n",
        "    buffer_size=4,\n",
        "    offload_per_layer=offload_per_layer,\n",
        ")\n",
        "\n",
        "\n",
        "attn_config = BaseQuantizeConfig(\n",
        "    nbits=4,\n",
        "    group_size=64,\n",
        "    quant_zero=True,\n",
        "    quant_scale=True,\n",
        ")\n",
        "attn_config[\"scale_quant_params\"][\"group_size\"] = 256\n",
        "\n",
        "\n",
        "ffn_config = BaseQuantizeConfig(\n",
        "    nbits=2,\n",
        "    group_size=16,\n",
        "    quant_zero=True,\n",
        "    quant_scale=True,\n",
        ")\n",
        "quant_config = QuantConfig(ffn_config=ffn_config, attn_config=attn_config)\n",
        "model = build_model(\n",
        "    device=device,\n",
        "    quant_config=quant_config,\n",
        "    offload_config=offload_config,\n",
        "    state_path=state_path,\n",
        ")\n",
        "\n",
        "# Build Mistral 7B with max input length 32256\n",
        "!python convert_checkpoint.py --model_dir ./mistralai/Mixtral-8x7B-v0.1 \\\n",
        "                              --output_dir ./tllm_checkpoint_1gpu_mistral \\\n",
        "                              --dtype float16\n",
        "\n",
        "# Build TensorRT engine\n",
        "!trtllm-build --checkpoint_dir ./tllm_checkpoint_1gpu_mistral \\\n",
        "               --output_dir ./tmp/mistral/7B/trt_engines/fp16/1-gpu/ \\\n",
        "               --gemm_plugin float16 \\\n",
        "               --max_input_len 32256\n",
        "\n",
        "# Run Mistral 7B fp16 inference with sliding window/cache size 4096\n",
        "!python3 run.py --max_output_len=50 \\\n",
        "                --tokenizer_dir ./tmp/llama/7B/ \\\n",
        "                --engine_dir=./tmp/llama/7B/trt_engines/fp16/1-gpu/ \\\n",
        "                --max_attention_window_size=4096\n",
        "\n",
        "# After running the above commands to build and run the model,\n",
        "# you already have the model variable from the build process.\n",
        "\n",
        "# You can then use this variable to initialize the new model with additional configuration\n",
        "\n",
        "# Define additional configuration parameters\n",
        "quant_config=quant_config,\n",
        "offload_config=offload_config,\n",
        "additional_state_path = './tllm_checkpoint_1gpu_mistral'  # Path to the previously built model\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 245
        },
        "id": "_mIpePTMFyRY",
        "outputId": "330e33db-c0a8-4b42-a390-9670e5dfb4d7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'tensorrt_llm'...\n",
            "fatal: repository 'https://github.com/NVIDIA/TensorRT-LLM/tree/0ab9d17a59c284d2de36889832fe9fc7c8697604/tensorrt_llm/' not found\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'AutoConfig' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-2b569e857904>\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'git clone https://github.com/NVIDIA/TensorRT-LLM/tree/0ab9d17a59c284d2de36889832fe9fc7c8697604/tensorrt_llm'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mconfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAutoConfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquantized_model_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mdevice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"cuda:0\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'AutoConfig' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Distilled Whisper, Gradio Stable Diffusion Cascade, and Voice AI for Ada including Model Running and Token saving (Short Term Memory)(Complete)"
      ],
      "metadata": {
        "id": "oj4UOJemXlAy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Distilled Whisper for User audio recognition Imports\n",
        "import sounddevice as sd\n",
        "import numpy as np\n",
        "import torch\n",
        "from whisper import load_model, transcribe\n",
        "from huggingface_hub import hf_hub_download\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, TextStreamer\n",
        "#Elevenlabs AI audio Imports\n",
        "import getpass\n",
        "import elevenlabs\n",
        "#Stable Diffusion Cascade Imports\n",
        "from gradio_client import Client\n",
        "!pip install gradio_client\n",
        "#Stable Diffusion Cascade Image Save\n",
        "from PIL import Image\n",
        "import os\n",
        "from IPython.display import display\n",
        "\n",
        "#Mixtral Model\n",
        "model_name = \"mistralai/Mixtral-8x7B-Instruct-v0.1\"  # Replace with your actual model name\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "#Load DistilWhisper model Unoptimized Original Model\n",
        "distil_large_v2 = hf_hub_download(repo_id=\"openai/whisper-large\", filename=\"model.pt\")\n",
        "model = load_model(model=\"large\", device=\"cuda\")\n",
        "\n",
        "\n",
        "def getIdentity(identityPath):\n",
        "    with open(identityPath, \"r\", encoding=\"utf-8\") as f:\n",
        "        identityContext = f.read()\n",
        "    return {\"role\": \"user\", \"content\": identityContext}\n",
        "\n",
        "def user_change_identity():\n",
        "    change = input(\"Change identity? Type 'yes' to change and 'no' for default (no change): \").strip().lower()\n",
        "    return change == \"yes\"\n",
        "\n",
        "def update_identity(identityPath):\n",
        "    print(\"Enter the new identity text (this will replace the current identity):\")\n",
        "    new_identity = input()\n",
        "    with open(identityPath, \"w\", encoding=\"utf-8\") as f:\n",
        "        f.write(new_identity)\n",
        "    print(\"Identity updated.\")\n",
        "\n",
        "def get_user_input_preference():\n",
        "    while True:\n",
        "        preference = input(\"Would you like to use the microphone or type your input? (mic/type): \").strip().lower()\n",
        "        if preference in [\"mic\", \"type\"]:\n",
        "            return preference\n",
        "        else:\n",
        "            print(\"Invalid input. Please type 'mic' for microphone or 'type' for typing your input.\")\n",
        "\n",
        "\n",
        "def record_audio(duration=5, samplerate=44100, channels=1):\n",
        "    \"\"\"Record audio from the microphone.\"\"\"\n",
        "    print(\"Recording...\")\n",
        "    recording = sd.rec(int(duration * samplerate), samplerate=samplerate, channels=channels, dtype='float32')\n",
        "    sd.wait()  # Wait until recording is finished\n",
        "    print(\"Recording stopped.\")\n",
        "    return recording\n",
        "\n",
        "def transcribe_audio(audio_data):\n",
        "    \"\"\"Transcribe audio data to text using DistilWhisper.\"\"\"\n",
        "    #Convert the audio data to a format compatible with DistilWhisper\n",
        "    audio_data = torch.tensor(audio_data, dtype=torch.float32).squeeze()  # Ensure audio data is in the correct shape\n",
        "    pred_out = transcribe(model, audio=audio_data, device=\"cuda\")\n",
        "    return pred_out[\"text\"]\n",
        "\n",
        "def ai_conversation(user_input, model_identity, past_key_values=None):\n",
        "    \"\"\"Generate a response from the AI model based on user input.\n",
        "\n",
        "    Args:\n",
        "        user_input (str): The text input from the user.\n",
        "        past_key_values (torch.Tensor, optional): Past key values for the model to maintain context. Defaults to None.\n",
        "\n",
        "    Returns:\n",
        "        str: The model's text response.\n",
        "        torch.Tensor: Updated past key values.\n",
        "    \"\"\"\n",
        "    prompt = [model_identity]  # This line ensures the model's identity is included in the prompt\n",
        "    prompt.append({\"role\": \"user\", \"content\": user_input})  # Append the user's input to the prompt\n",
        "\n",
        "    input_ids = tokenizer(user_input, return_tensors=\"pt\").input_ids.to(device)\n",
        "\n",
        "    # Create attention mask based on input_ids\n",
        "    attention_mask = torch.ones(input_ids.shape, dtype=torch.long, device=device)\n",
        "\n",
        "    # Generate model response\n",
        "    result = model.generate(\n",
        "        input_ids=input_ids,\n",
        "        attention_mask=attention_mask,\n",
        "        past_key_values=past_key_values,\n",
        "        max_length=1000,  # Adjusted as per requirement\n",
        "        pad_token_id=tokenizer.eos_token_id,\n",
        "        do_sample=True,\n",
        "        temperature=0.9,\n",
        "        top_p=0.9,\n",
        "        max_new_tokens=512,\n",
        "        return_dict_in_generate=True,\n",
        "        output_hidden_states=False  # Adjust as needed\n",
        "    )\n",
        "\n",
        "    # Extract the generated text\n",
        "    response_text = tokenizer.decode(result.sequences[0], skip_special_tokens=True)\n",
        "\n",
        "    # Update past_key_values if needed for continuity in conversation\n",
        "    new_past_key_values = result.past_key_values if 'past_key_values' in result else None\n",
        "\n",
        "    return response_text, new_past_key_values\n",
        "\n",
        "def main():\n",
        "    identityPath = \"characterConfig/Pina/identity.txt\"\n",
        "    if user_change_identity():\n",
        "        update_identity(identityPath)\n",
        "    model_identity = getIdentity(identityPath)\n",
        "\n",
        "    past_key_values = None  # Initialize past_key_values\n",
        "\n",
        "    while True:\n",
        "        preference = get_user_input_preference()\n",
        "\n",
        "        if preference == \"mic\":\n",
        "            if not user_decision_to_record():\n",
        "                print(\"Recording aborted by the user.\")\n",
        "                continue\n",
        "\n",
        "            print(\"Please start speaking. Recording will stop after a silence.\")\n",
        "            audio_data = record_audio()\n",
        "\n",
        "            print(\"Transcribing...\")\n",
        "            transcription = transcribe_audio(audio_data)\n",
        "        else:  # User prefers typing\n",
        "            print(\"User: \", end=\"\")\n",
        "            transcription = input()\n",
        "\n",
        "        print(f\"User said: {transcription}\")  # User's input (typed or transcribed) is printed here\n",
        "\n",
        "        # Ensure you pass model_identity to ai_conversation\n",
        "        model_response, past_key_values =  ai_conversation(transcription, model_identity, past_key_values)\n",
        "        print(f\"AI responded: {model_response}\")  # AI's response is printed here\n",
        "\n",
        "        # Process AI response for image generation and voice output\n",
        "        process_ai_response(model_response)\n",
        "        handle_voice_output(model_response)\n",
        "\n",
        "def process_ai_response(model_response):\n",
        "    # Check for \"draw start\" phrase in model_response for image generation\n",
        "    if \"draw start\" in model_response.lower():\n",
        "        print(\"Starting image generation...\")\n",
        "        generate_image(model_response)  # Call generate_image with the current model_response\n",
        "    else:\n",
        "        print(\"No image generation triggered.\")\n",
        "\n",
        "\n",
        "def handle_voice_output(model_response):\n",
        "    if user_decision_for_voice_output():\n",
        "        print(\"Generating AI voice output...\")\n",
        "        elevenlabs_tts(model_response)  # model_response is the AI-generated text you wish to convert to speech\n",
        "    else:\n",
        "        print(\"AI voice output disabled by the user.\")\n",
        "\n",
        "api_key = None\n",
        "voice_id = None\n",
        "\n",
        "def get_elevenlabs_credentials():\n",
        "    global api_key, voice_id\n",
        "    if api_key is None:\n",
        "        api_key = getpass.getpass('Enter your Elevenlabs API key: ')\n",
        "    if voice_id is None:\n",
        "        voice_id = getpass.getpass('Enter your Elevenlabs Voice ID: ')\n",
        "    elevenlabs.set_api_key(api_key)\n",
        "    return voice_id\n",
        "\n",
        "def elevenlabs_tts(text, model='eleven_monolingual_v1'):\n",
        "    voice_id = get_elevenlabs_credentials()\n",
        "    audio = elevenlabs.generate(\n",
        "        text=text,\n",
        "        voice=voice_id,\n",
        "        model=model\n",
        "    )\n",
        "    elevenlabs.play(audio)\n",
        "\n",
        "\n",
        "#Stable Diffusion Cascade\n",
        "\n",
        "def generate_image(model_response):\n",
        "  PosPrompt = model_response + \"bueatiful, insanely detailed, 8k\"\n",
        "  client = Client(\"multimodalart/stable-cascade\")\n",
        "  result = client.predict(\n",
        "\t\tPosPrompt,\t# str  in 'Prompt' Textbox component\n",
        "\t\t\"nudity, NSFW, bad hands, bad anatomy, ((((ugly)))), (((duplicate))), ((morbid)), ((mutilated)), out of frame, extra fingers, mutated hands, ((poorly drawn hands)), ((poorly drawn face)), (((mutation))), (((deformed))), ((ugly)), blurry, ((bad anatomy)), (((bad proportions))), ((extra limbs)), cloned face, (((disfigured))), out of frame, ugly, extra limbs, (bad anatomy), gross proportions, (malformed limbs), ((missing arms)), ((missing legs)), (((extra arms))), (((extra legs))), mutated hands, (fused fingers), (too many fingers), (((long neck)))\",\t# str  in 'Negative prompt' Textbox component\n",
        "\t\t0,\t# float (numeric value between 0 and 2147483647) in 'Seed' Slider component\n",
        "\t\t1024,\t# float (numeric value between 1024 and 1536) in 'Width' Slider component\n",
        "\t\t1024,\t# float (numeric value between 1024 and 1536) in 'Height' Slider component\n",
        "\t\t10,\t# float (numeric value between 10 and 30) in 'Prior Inference Steps' Slider component\n",
        "\t\t7.5,\t# float (numeric value between 0 and 20) in 'Prior Guidance Scale' Slider component\n",
        "\t\t4,\t# float (numeric value between 4 and 12) in 'Decoder Inference Steps' Slider component\n",
        "\t\t0,\t# float (numeric value between 0 and 0) in 'Decoder Guidance Scale' Slider component\n",
        "\t\t1,\t# float (numeric value between 1 and 2) in 'Number of Images' Slider component\n",
        "\t\tapi_name=\"/run\"\n",
        ")\n",
        "print(result)\n",
        "display_and_save_image(image_file_path, output_dir=\"saved_images\")\n",
        "\n",
        "\n",
        "def display_and_save_image(image_file_path, output_dir=\"saved_images\"):\n",
        "    \"\"\"\n",
        "    Loads, displays, and optionally saves an image to a specified directory.\n",
        "\n",
        "    Parameters:\n",
        "    - image_file_path: str, the path to the image file.\n",
        "    - output_dir: str, the directory where the image will be saved (optional).\n",
        "    \"\"\"\n",
        "    # Ensure the file exists\n",
        "    if os.path.exists(image_file_path):\n",
        "        # Load and display the image\n",
        "        image = Image.open(image_file_path)\n",
        "        display(image)\n",
        "\n",
        "        # Optionally, save the image to a new location\n",
        "        os.makedirs(output_dir, exist_ok=True)  # Ensure the output directory exists\n",
        "        output_path = os.path.join(output_dir, os.path.basename(image_file_path))\n",
        "        image.save(output_path)\n",
        "\n",
        "        print(f\"Image has been saved to: {output_path}\")\n",
        "    else:\n",
        "        print(f\"Image not found at: {image_file_path}\")\n",
        "\n",
        "# Example usage:\n",
        "# result = \"path/to/your/image.jpg\"  # Define the path to the generated image\n",
        "# display_and_save_image(result)\n",
        "\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "      main()"
      ],
      "metadata": {
        "id": "UMnBvRtvXjfA",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 384
        },
        "outputId": "2bcb689b-e93d-41f9-efa6-e726f7a5fd01"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'sounddevice'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-be751e9a460c>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#Distilled Whisper for User audio recognition Imports\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0msounddevice\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mwhisper\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mload_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtranscribe\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'sounddevice'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Distilled Whisper, Gradio Stable Diffusion Cascade, and Voice AI for Ada including Model Running and Token saving (Short Term Memory and Long Term Memory with Zep) This Model Requires a A100 GPU to run at its most Optimal but other GPUs should work as well; the code execution just may not be as fast."
      ],
      "metadata": {
        "id": "NvkSHa6Iira-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Distilled Whisper for User audio recognition Imports\n",
        "import sounddevice as sd\n",
        "import numpy as np\n",
        "import torch\n",
        "from whisper import load_model, transcribe\n",
        "from huggingface_hub import hf_hub_download\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, TextStreamer\n",
        "#Elevenlabs AI audio Imports\n",
        "import getpass\n",
        "import elevenlabs\n",
        "#Stable Diffusion Cascade Imports\n",
        "from gradio_client import Client\n",
        "!pip install gradio_client\n",
        "#Stable Diffusion Cascade Image Save\n",
        "from PIL import Image\n",
        "import os\n",
        "from IPython.display import display\n",
        "\n",
        "#Mixtral Model\n",
        "model_name = \"mistralai/Mixtral-8x7B-Instruct-v0.1\"  # Replace with your actual model name\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "#Load DistilWhisper model Unoptimized Original Model\n",
        "distil_large_v2 = hf_hub_download(repo_id=\"openai/whisper-large\", filename=\"model.pt\")\n",
        "model = load_model(model=\"large\", device=\"cuda\")\n",
        "\n",
        "#Zep Code Imports\n",
        "\n",
        "from uuid import uuid4\n",
        "\n",
        "from langchain.agents import AgentType, Tool,  load_tools, initialize_agent\n",
        "from langchain.memory import ZepMemory\n",
        "from langchain.retrievers import ZepRetriever\n",
        "from langchain.schema import AIMessage, HumanMessage\n",
        "from langchain_community.utilities import WikipediaAPIWrapper\n",
        "from langchain_openai import OpenAI\n",
        "\n",
        "# Set this to your Zep server URL\n",
        "ZEP_API_URL = \"http://localhost:8000\"\n",
        "\n",
        "session_id = str(uuid4())  # This is a unique identifier for the user\n",
        "\n",
        "# Provide your Zep API key. Note that this is optional. See https://docs.getzep.com/deployment/auth\n",
        "\n",
        "#zep_api_key = getpass.getpass()\n",
        "\n",
        "##Set-up and Initialize the ZEP Agent##\n",
        "tools = load_tools([\"google-serper\"], llm=llm)\n",
        "\n",
        "\n",
        "# Set up Zep Chat History\n",
        "memory = ZepMemory(\n",
        "    session_id=session_id,\n",
        "    url=ZEP_API_URL,\n",
        "    api_key=zep_api_key,\n",
        "    memory_key=\"chat_history\",\n",
        ")\n",
        "\n",
        "# Initialize the agent\n",
        " model, device = initialize_mixtral_model(\n",
        "    quantized_model_name=\"lavawolfiee/Mixtral-8x7B-Instruct-v0.1-offloading-demo\",\n",
        "    state_path=\"Mixtral-8x7B-Instruct-v0.1-offloading-demo\",\n",
        "    offload_per_layer=4  # Adjust based on your GPU VRAM\n",
        ")\n",
        "\n",
        "llm = model  # Assign only the model to llm for usage in the agent chain\n",
        "\n",
        "                                                                               #OpenAI(temperature=0, openai_api_key=openai_key) - Original LLM\n",
        "agent_chain = initialize_agent(\n",
        "    tools,\n",
        "    llm,\n",
        "    agent=AgentType.CONVERSATIONAL_REACT_DESCRIPTION,\n",
        "    verbose=True,\n",
        "    memory=memory,\n",
        ")\n",
        "\n",
        "\n",
        "def getIdentity(identityPath):\n",
        "    with open(identityPath, \"r\", encoding=\"utf-8\") as f:\n",
        "        identityContext = f.read()\n",
        "    return {\"role\": \"user\", \"content\": identityContext}\n",
        "\n",
        "def user_change_identity():\n",
        "    change = input(\"Change identity? Type 'yes' to change and 'no' for default (no change): \").strip().lower()\n",
        "    return change == \"yes\"\n",
        "\n",
        "def update_identity(identityPath):\n",
        "    print(\"Enter the new identity text (this will replace the current identity):\")\n",
        "    new_identity = input()\n",
        "    with open(identityPath, \"w\", encoding=\"utf-8\") as f:\n",
        "        f.write(new_identity)\n",
        "    print(\"Identity updated.\")\n",
        "\n",
        "def get_user_input_preference():\n",
        "    while True:\n",
        "        preference = input(\"Would you like to use the microphone or type your input? (mic/type): \").strip().lower()\n",
        "        if preference in [\"mic\", \"type\"]:\n",
        "            return preference\n",
        "        else:\n",
        "            print(\"Invalid input. Please type 'mic' for microphone or 'type' for typing your input.\")\n",
        "\n",
        "\n",
        "def record_audio(duration=5, samplerate=44100, channels=1):\n",
        "    \"\"\"Record audio from the microphone.\"\"\"\n",
        "    print(\"Recording...\")\n",
        "    recording = sd.rec(int(duration * samplerate), samplerate=samplerate, channels=channels, dtype='float32')\n",
        "    sd.wait()  # Wait until recording is finished\n",
        "    print(\"Recording stopped.\")\n",
        "    return recording\n",
        "\n",
        "def transcribe_audio(audio_data):\n",
        "    \"\"\"Transcribe audio data to text using DistilWhisper.\"\"\"\n",
        "    #Convert the audio data to a format compatible with DistilWhisper\n",
        "    audio_data = torch.tensor(audio_data, dtype=torch.float32).squeeze()  # Ensure audio data is in the correct shape\n",
        "    pred_out = transcribe(model, audio=audio_data, device=\"cuda\")\n",
        "    return pred_out[\"text\"]\n",
        "\n",
        "#Run the AI Agent\n",
        "agent_chain.run(\n",
        "    def ai_conversation(user_input, model_identity, past_key_values=None):\n",
        "    \"\"\"Generate a response from the AI model based on user input.\n",
        "\n",
        "    Args:\n",
        "        user_input (str): The text input from the user.\n",
        "        past_key_values (torch.Tensor, optional): Past key values for the model to maintain context. Defaults to None.\n",
        "\n",
        "    Returns:\n",
        "        str: The model's text response.\n",
        "        torch.Tensor: Updated past key values.\n",
        "    \"\"\"\n",
        "    prompt = [model_identity]  # This line ensures the model's identity is included in the prompt #Might need to change this to input\n",
        "    prompt.append({\"role\": \"user\", \"content\": user_input})  # Append the user's input to the prompt\n",
        "    input = prompt\n",
        "    input_ids = tokenizer(user_input, return_tensors=\"pt\").input_ids.to(device)\n",
        "\n",
        "    # Create attention mask based on input_ids\n",
        "    attention_mask = torch.ones(input_ids.shape, dtype=torch.long, device=device)\n",
        "\n",
        "    # Generate model response\n",
        "    result = model.generate(\n",
        "        input_ids=input_ids,\n",
        "        attention_mask=attention_mask,\n",
        "        past_key_values=past_key_values,\n",
        "        max_length=1000,  # Adjusted as per requirement\n",
        "        pad_token_id=tokenizer.eos_token_id,\n",
        "        do_sample=True,\n",
        "        temperature=0.9,\n",
        "        top_p=0.9,\n",
        "        max_new_tokens=512,\n",
        "        return_dict_in_generate=True,\n",
        "        output_hidden_states=False  # Adjust as needed\n",
        "    )\n",
        "\n",
        "    # Extract the generated text\n",
        "    response_text = tokenizer.decode(result.sequences[0], skip_special_tokens=True)\n",
        "\n",
        "    # Update past_key_values if needed for continuity in conversation\n",
        "    new_past_key_values = result.past_key_values if 'past_key_values' in result else None\n",
        "\n",
        "    return response_text, new_past_key_values\n",
        "\n",
        ")\n",
        "\n",
        "\n",
        "    # Extract the generated text\n",
        "    response_text = tokenizer.decode(result.sequences[0], skip_special_tokens=True)\n",
        "\n",
        "    # Update past_key_values if needed for continuity in conversation\n",
        "    new_past_key_values = result.past_key_values if 'past_key_values' in result else None\n",
        "\n",
        "    return response_text, new_past_key_values\n",
        "\n",
        "def main():\n",
        "    identityPath = \"characterConfig/Pina/identity.txt\"\n",
        "    if user_change_identity():\n",
        "        update_identity(identityPath)\n",
        "    model_identity = getIdentity(identityPath)\n",
        "\n",
        "    past_key_values = None  # Initialize past_key_values\n",
        "\n",
        "    while True:\n",
        "        preference = get_user_input_preference()\n",
        "\n",
        "        if preference == \"mic\":\n",
        "            if not user_decision_to_record():\n",
        "                print(\"Recording aborted by the user.\")\n",
        "                continue\n",
        "\n",
        "            print(\"Please start speaking. Recording will stop after a silence.\")\n",
        "            audio_data = record_audio()\n",
        "\n",
        "            print(\"Transcribing...\")\n",
        "            transcription = transcribe_audio(audio_data)\n",
        "        else:  # User prefers typing\n",
        "            print(\"User: \", end=\"\")\n",
        "            transcription = input()\n",
        "\n",
        "        print(f\"User said: {transcription}\")  # User's input (typed or transcribed) is printed here\n",
        "\n",
        "        # Ensure you pass model_identity to ai_conversation\n",
        "        model_response, past_key_values = agent_chain.run(ai_conversation(transcription, model_identity, past_key_values))\n",
        "        print(f\"AI responded: {model_response}\")  # AI's response is printed here\n",
        "\n",
        "        # Process AI response for image generation and voice output\n",
        "        process_ai_response(model_response)\n",
        "        handle_voice_output(model_response)\n",
        "\n",
        "\n",
        "        # After response, provide options to user\n",
        "        user_action = input(\"Enter 'print' to display chat history, 'delete' to clear memory, or 'continue' to keep chatting: \").lower()\n",
        "        if user_action == 'print':\n",
        "            print_messages(memory.chat_memory.messages)\n",
        "        elif user_action == 'delete':\n",
        "            memory.clear()\n",
        "            print(\"Memory cleared.\")\n",
        "        elif user_action == 'continue':\n",
        "            continue\n",
        "\n",
        "def process_ai_response(model_response):\n",
        "    # Check for \"draw start\" phrase in model_response for image generation\n",
        "    if \"draw start\" in model_response.lower():\n",
        "        print(\"Starting image generation...\")\n",
        "        generate_image(model_response)  # Call generate_image with the current model_response\n",
        "    else:\n",
        "        print(\"No image generation triggered.\")\n",
        "\n",
        "\n",
        "def handle_voice_output(model_response):\n",
        "    if user_decision_for_voice_output():\n",
        "        print(\"Generating AI voice output...\")\n",
        "        elevenlabs_tts(model_response)  # model_response is the AI-generated text you wish to convert to speech\n",
        "    else:\n",
        "        print(\"AI voice output disabled by the user.\")\n",
        "\n",
        "api_key = None\n",
        "voice_id = None\n",
        "\n",
        "def get_elevenlabs_credentials():\n",
        "    global api_key, voice_id\n",
        "    if api_key is None:\n",
        "        api_key = getpass.getpass('Enter your Elevenlabs API key: ')\n",
        "    if voice_id is None:\n",
        "        voice_id = getpass.getpass('Enter your Elevenlabs Voice ID: ')\n",
        "    elevenlabs.set_api_key(api_key)\n",
        "    return voice_id\n",
        "\n",
        "def elevenlabs_tts(text, model='eleven_monolingual_v1'):\n",
        "    voice_id = get_elevenlabs_credentials()\n",
        "    audio = elevenlabs.generate(\n",
        "        text=text,\n",
        "        voice=voice_id,\n",
        "        model=model\n",
        "    )\n",
        "    elevenlabs.play(audio)\n",
        "\n",
        "\n",
        "#Stable Diffusion Cascade\n",
        "\n",
        "def generate_image(model_response):\n",
        "PosPrompt = model_response + \"bueatiful, insanely detailed, 8k\"\n",
        "client = Client(\"multimodalart/stable-cascade\")\n",
        "result = client.predict(\n",
        "\t\tPosPrompt,\t# str  in 'Prompt' Textbox component\n",
        "\t\t\"nudity, NSFW, bad hands, bad anatomy, ((((ugly)))), (((duplicate))), ((morbid)), ((mutilated)), out of frame, extra fingers, mutated hands, ((poorly drawn hands)), ((poorly drawn face)), (((mutation))), (((deformed))), ((ugly)), blurry, ((bad anatomy)), (((bad proportions))), ((extra limbs)), cloned face, (((disfigured))), out of frame, ugly, extra limbs, (bad anatomy), gross proportions, (malformed limbs), ((missing arms)), ((missing legs)), (((extra arms))), (((extra legs))), mutated hands, (fused fingers), (too many fingers), (((long neck)))\",\t# str  in 'Negative prompt' Textbox component\n",
        "\t\t0,\t# float (numeric value between 0 and 2147483647) in 'Seed' Slider component\n",
        "\t\t1024,\t# float (numeric value between 1024 and 1536) in 'Width' Slider component\n",
        "\t\t1024,\t# float (numeric value between 1024 and 1536) in 'Height' Slider component\n",
        "\t\t10,\t# float (numeric value between 10 and 30) in 'Prior Inference Steps' Slider component\n",
        "\t\t7.5,\t# float (numeric value between 0 and 20) in 'Prior Guidance Scale' Slider component\n",
        "\t\t4,\t# float (numeric value between 4 and 12) in 'Decoder Inference Steps' Slider component\n",
        "\t\t0,\t# float (numeric value between 0 and 0) in 'Decoder Guidance Scale' Slider component\n",
        "\t\t1,\t# float (numeric value between 1 and 2) in 'Number of Images' Slider component\n",
        "\t\tapi_name=\"/run\"\n",
        ")\n",
        "print(result)\n",
        "display_and_save_image(image_file_path, output_dir=\"saved_images\")\n",
        "\n",
        "\n",
        "def display_and_save_image(image_file_path, output_dir=\"saved_images\"):\n",
        "    \"\"\"\n",
        "    Loads, displays, and optionally saves an image to a specified directory.\n",
        "\n",
        "    Parameters:\n",
        "    - image_file_path: str, the path to the image file.\n",
        "    - output_dir: str, the directory where the image will be saved (optional).\n",
        "    \"\"\"\n",
        "    # Ensure the file exists\n",
        "    if os.path.exists(image_file_path):\n",
        "        # Load and display the image\n",
        "        image = Image.open(image_file_path)\n",
        "        display(image)\n",
        "\n",
        "        # Optionally, save the image to a new location\n",
        "        os.makedirs(output_dir, exist_ok=True)  # Ensure the output directory exists\n",
        "        output_path = os.path.join(output_dir, os.path.basename(image_file_path))\n",
        "        image.save(output_path)\n",
        "\n",
        "        print(f\"Image has been saved to: {output_path}\")\n",
        "    else:\n",
        "        print(f\"Image not found at: {image_file_path}\")\n",
        "\n",
        "# Example usage:\n",
        "# result = \"path/to/your/image.jpg\"  # Define the path to the generated image\n",
        "# display_and_save_image(result)\n",
        "\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "      main()"
      ],
      "metadata": {
        "id": "DUoKlplPi4rj",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "outputId": "19c35981-111e-4be8-8d74-f0d95c1786f3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "IndentationError",
          "evalue": "unexpected indent (<ipython-input-7-d08dc7d53c79>, line 63)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-7-d08dc7d53c79>\"\u001b[0;36m, line \u001b[0;32m63\u001b[0m\n\u001b[0;31m    model, device = initialize_mixtral_model(\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m unexpected indent\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Zep Memory and Document Reading Storage for Ada AI (Mixtral) Unused"
      ],
      "metadata": {
        "id": "7zx03DjKpyFf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Zep Code Properly organized\n",
        "\n",
        "from uuid import uuid4\n",
        "\n",
        "from langchain.agents import AgentType, Tool,  load_tools, initialize_agent\n",
        "from langchain.memory import ZepMemory\n",
        "from langchain.retrievers import ZepRetriever\n",
        "from langchain.schema import AIMessage, HumanMessage\n",
        "from langchain_community.utilities import WikipediaAPIWrapper\n",
        "from langchain_openai import OpenAI\n",
        "\n",
        "# Set this to your Zep server URL\n",
        "ZEP_API_URL = \"http://localhost:8000\"\n",
        "\n",
        "session_id = str(uuid4())  # This is a unique identifier for the user\n",
        "\n",
        "# Provide your Zep API key. Note that this is optional. See https://docs.getzep.com/deployment/auth\n",
        "\n",
        "#zep_api_key = getpass.getpass()\n",
        "\n",
        "##Set-up and Initialize the ZEP Agent##\n",
        "tools = load_tools([\"google-serper\"], llm=llm)\n",
        "\n",
        "\n",
        "# Set up Zep Chat History\n",
        "memory = ZepMemory(\n",
        "    session_id=session_id,\n",
        "    url=ZEP_API_URL,\n",
        "    api_key=zep_api_key,\n",
        "    memory_key=\"chat_history\",\n",
        ")\n",
        "\n",
        "# Initialize the agent\n",
        " model, device = initialize_mixtral_model(\n",
        "    quantized_model_name=\"lavawolfiee/Mixtral-8x7B-Instruct-v0.1-offloading-demo\",\n",
        "    state_path=\"Mixtral-8x7B-Instruct-v0.1-offloading-demo\",\n",
        "    offload_per_layer=4  # Adjust based on your GPU VRAM\n",
        ")\n",
        "\n",
        "llm = model  # Assign only the model to llm for usage in the agent chain\n",
        "\n",
        "                                                                               #OpenAI(temperature=0, openai_api_key=openai_key) - Original LLM\n",
        "agent_chain = initialize_agent(\n",
        "    tools,\n",
        "    llm,\n",
        "    agent=AgentType.CONVERSATIONAL_REACT_DESCRIPTION,\n",
        "    verbose=True,\n",
        "    memory=memory,\n",
        ")\n",
        "\n",
        "\n",
        "for msg in test_history:\n",
        "    memory.chat_memory.add_message(\n",
        "        (\n",
        "            HumanMessage(content=msg[\"content\"])\n",
        "            if msg[\"role\"] == \"human\"\n",
        "            else AIMessage(content=msg[\"content\"])\n",
        "        ),\n",
        "        metadata=msg.get(\"metadata\", {}),\n",
        "    )\n",
        "\n",
        "#Run the AI Agent\n",
        "agent_chain.run(\n",
        "    def ai_conversation(user_input, model_identity, past_key_values=None):\n",
        "    \"\"\"Generate a response from the AI model based on user input.\n",
        "\n",
        "    Args:\n",
        "        user_input (str): The text input from the user.\n",
        "        past_key_values (torch.Tensor, optional): Past key values for the model to maintain context. Defaults to None.\n",
        "\n",
        "    Returns:\n",
        "        str: The model's text response.\n",
        "        torch.Tensor: Updated past key values.\n",
        "    \"\"\"\n",
        "    prompt = [model_identity]  # This line ensures the model's identity is included in the prompt #Might need to change this to input\n",
        "    prompt.append({\"role\": \"user\", \"content\": user_input})  # Append the user's input to the prompt\n",
        "    input = prompt\n",
        "    input_ids = tokenizer(user_input, return_tensors=\"pt\").input_ids.to(device)\n",
        "\n",
        "    # Create attention mask based on input_ids\n",
        "    attention_mask = torch.ones(input_ids.shape, dtype=torch.long, device=device)\n",
        "\n",
        "    # Generate model response\n",
        "    result = model.generate(\n",
        "        input_ids=input_ids,\n",
        "        attention_mask=attention_mask,\n",
        "        past_key_values=past_key_values,\n",
        "        max_length=1000,  # Adjusted as per requirement\n",
        "        pad_token_id=tokenizer.eos_token_id,\n",
        "        do_sample=True,\n",
        "        temperature=0.9,\n",
        "        top_p=0.9,\n",
        "        max_new_tokens=512,\n",
        "        return_dict_in_generate=True,\n",
        "        output_hidden_states=False  # Adjust as needed\n",
        "    )\n",
        "\n",
        "    # Extract the generated text\n",
        "    response_text = tokenizer.decode(result.sequences[0], skip_special_tokens=True)\n",
        "\n",
        "    # Update past_key_values if needed for continuity in conversation\n",
        "    new_past_key_values = result.past_key_values if 'past_key_values' in result else None\n",
        "\n",
        "    return response_text, new_past_key_values\n",
        "\n",
        ")\n",
        "\n",
        "#Print a Summary of the AI conversation\n",
        "def print_messages(messages):\n",
        "    for m in messages:\n",
        "        print(m.type, \":\\n\", m.dict())\n",
        "\n",
        "\n",
        "print(memory.chat_memory.zep_summary)\n",
        "print(\"\\n\")\n",
        "print_messages(memory.chat_memory.messages)\n",
        "\n",
        "#Search the Zep Memory for a specific memory\n",
        "retriever = ZepRetriever(\n",
        "    session_id=session_id,\n",
        "    url=ZEP_API_URL,\n",
        "    api_key=zep_api_key,\n",
        ")\n",
        "\n",
        "search_results = memory.chat_memory.search(\"who are some famous women sci-fi authors?\")\n",
        "for r in search_results:\n",
        "    if r.dist > 0.8:  # Only print results with similarity of 0.8 or higher\n",
        "        print(r.message, r.dist)\n",
        "\n",
        "#Delete All Session Memories##\n",
        " def clear(self) -> None:\n",
        "        \"\"\"Clear session memory from Zep. Note that Zep is long-term storage for memory\n",
        "        and this is not advised unless you have specific data retention requirements.\n",
        "        \"\"\"\n",
        "        try:\n",
        "            self.zep_client.delete_memory(self.session_id)\n",
        "        except NotFoundError:\n",
        "            logger.warning(\n",
        "                f\"Session {self.session_id} not found in Zep. Skipping delete.\"\n",
        "            )"
      ],
      "metadata": {
        "id": "0fehLINTso8b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Zep Vector Memory Storage which creates a document collection that can be added to the model's long term memory for context or searches.\n",
        "\n",
        "from llama_index.vector_stores import ZepVectorStore\n",
        "zep_api_url = \"http://localhost:8000\"\n",
        "zep_api_key = \"<optional_jwt_token>\"\n",
        "collection_name = \"document\" # The name of a new or existing collection\n",
        "embedding_dimensions = 1536 # the dimensions of the embedding model you intend to use\n",
        "vector_store = ZepVectorStore(\n",
        "  api_url=zep_api_url,\n",
        "  api_key=zep_api_key,\n",
        "  collection_name=collection_name,\n",
        "  embedding_dimensions=embedding_dimensions\n",
        ")\n",
        "\n",
        "from llama_index import VectorStoreIndex, SimpleDirectoryReader\n",
        "from llama_index.storage.storage_context import StorageContext\n",
        "\n",
        "documents = SimpleDirectoryReader(\"./document_calculating_engine/\").load_data()\n",
        "\n",
        "storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
        "index = VectorStoreIndex.from_documents(documents, storage_context=storage_context)\n",
        "\n",
        "query = \"Roleplaying Games\" #AI Prompt\n",
        "\n",
        "query_engine = index.as_query_engine()\n",
        "response = query_engine.query(query)\n",
        "\n",
        "\n",
        "print(str(response))\n",
        "\n",
        "rom llama_index.schema import TextNode\n",
        "from llama_index.vector_stores.types import ExactMatchFilter, MetadataFilters\n",
        "\n",
        "nodes = [\n",
        "   TextNode(\n",
        "       text=\"PARTl Creating a Character for Dungeons and Dragons Record your level on your character sheet. If you're starting at a higher level, record the additional elements your class gives you for your levels past 1st. Also record your experience points. A I st-level character has 0 XP. A higher-level character typically begins with the minimum amount ofXP required to reach that level (see \"Beyond lst Level\" later in this chapter). HIT POINTS AND HIT DICE. Your character's hit points define how tough your character is in combat and other dangerous situations. Your hit points are determined by your Hit Dice (short for Hit Point Dice). ABILITY SCORE SUMMARY Strength Measures: Natural athleticism, bodily power Important for: Barbarian, fighter, paladin Racial Increases: Mountain dwarf (+2) Half-ore (+2) Dragonborn (+2) Human (+l) Dexterity Measures: Physical agility, reflexes, balance, poise Important for: Monk, ranger, rogue Racial Increases: Elf (+2) Forest gnome (+1) Halfing (+2) Human (+1) Constitution Measures: Health, stamina, vital force Important for: Everyone Racial Increases: Dwarf(+2) Half-ore (+1) Stout halfing (+ 1) Human (+1) Rock gnome (+1).\"\n",
        "       metadata={\n",
        "           \"topic\": \"Dungeons and Dragons Roleplaying Game Character Creation\",\n",
        "           \"entities\": \"Game\",\n",
        "       },\n",
        "   ),\n",
        "   #TextNode( Another Placeholder Example of TextNode#\n",
        "       #text=\"Within the limits of the lunar orbit there are not less than one thousand stars, which are so situated as to be in the moon's path, and therefore to exhibit, at some period or other, those desirable occultations.\",\n",
        "       #metadata={\n",
        "           #\"topic\": \"astronomy\",\n",
        "           #\"entities\": \"moon\",\n",
        "       #},\n",
        "   #),\n",
        "]\n",
        "\n",
        "\n",
        "storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
        "index = VectorStoreIndex(nodes, storage_context=storage_context)\n",
        "\n",
        "\n",
        "filters = MetadataFilters(filters=[ExactMatchFilter(key=\"topic\", value=\"astronomy\")])\n",
        "\n",
        "\n",
        "retriever = index.as_retriever(filters=filters)\n",
        "result = retriever.retrieve(\"What is the structure of our galaxy?\") #Result from TextNode\n",
        "\n",
        "\n",
        "for r in result:\n",
        "   print(\"\\n\", r.node.text, r.score)"
      ],
      "metadata": {
        "id": "goraboZzNeas"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Run the base Ada model without user voice and AI voice features.(Optional-Above Code is the Complete Ver. I recommend using that build over this one.)"
      ],
      "metadata": {
        "id": "Z4hBFYtPTUzT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import TextStreamer\n",
        "\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "streamer = TextStreamer(tokenizer, skip_prompt=True, skip_special_tokens=True)\n",
        "past_key_values = None\n",
        "sequence = None\n",
        "\n",
        "seq_len = 0\n",
        "while True:\n",
        "  print(\"User: \", end=\"\")\n",
        "  user_input = input()\n",
        "  print(\"\\n\")\n",
        "\n",
        "  user_entry = dict(role=\"user\", content=user_input)\n",
        "  input_ids = tokenizer.apply_chat_template([user_entry], return_tensors=\"pt\").to(device)\n",
        "\n",
        "  if past_key_values is None:\n",
        "    attention_mask = torch.ones_like(input_ids)\n",
        "  else:\n",
        "    seq_len = input_ids.size(1) + past_key_values[0][0][0].size(1)\n",
        "    attention_mask = torch.ones([1, seq_len - 1], dtype=torch.int, device=device)\n",
        "\n",
        "  print(\"Mixtral: \", end=\"\")\n",
        "  result = model.generate(\n",
        "    input_ids=input_ids,\n",
        "    attention_mask=attention_mask,\n",
        "    past_key_values=past_key_values,\n",
        "    streamer=streamer,\n",
        "    do_sample=True,\n",
        "    temperature=0.9,\n",
        "    top_p=0.9,\n",
        "    max_new_tokens=512,\n",
        "    pad_token_id=tokenizer.eos_token_id,\n",
        "    return_dict_in_generate=True,\n",
        "    output_hidden_states=True,\n",
        "  )\n",
        "  print(\"\\n\")\n",
        "\n",
        "  sequence = result[\"sequences\"]\n",
        "  past_key_values = result[\"past_key_values\"]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 712,
          "referenced_widgets": [
            "d020265098354ca09ee75939288cd60f",
            "4f1df81fb8e44a269959ad6050470e71",
            "0102f2f4bf0c422892bca75eda4d5f9b",
            "b175f4ad0dc54125a7048d73a4b2f296",
            "4f55d83e2c54448a82f5710f8f263fad",
            "48bcea2cd2d9409ca78e562e7727932a",
            "a2aaebdf246a4f05b5dc1c6b52c11a48",
            "da73133c3d7945ec9e1d732431701bad",
            "9a97e76872d1405ca72815477223c577",
            "e89fb7d2b2ec47ad8390692f0770554b",
            "2fbee9f5335d44afa27ee3d9f344e569",
            "f33b909ccc4c42f7a1e3e5eeac600c71",
            "f9f52155474941ca92f845b0434f93cd",
            "2b1c39f3fab54ecda0adec190ad65f32",
            "5673424ad0124af9908345bb36d143f2",
            "c33d456662064be8be72b05112cf2647",
            "36bd94d5a28d452a9ebc27ed6bbee6c9",
            "fd85df9f9a824aacb07ac178ee61980e",
            "ef482343e9cd44e79f530b808b33ccd0",
            "ccf376b450d04301814f2bc6fd62495c",
            "504507adfa324c9cb7817fbd9794c997",
            "dee63f55a5ac468c9c368c6fe0b74dfe",
            "ac7ec3817dcf4f8a8676f7d653f1da67",
            "7700c48e9fd0468dbdda9e260d88d093",
            "db0b424831224ce58be5c465ae9bdba2",
            "92edf25a37f74ab2a62c80ef21951df7",
            "25ce6c7a58174438a5e54758e6dda7b7",
            "75c853e8fc3942ad95411540616aba70",
            "86fe97474733444f864e01552ded08d1",
            "05b797a215d94489a65647c202d01c26",
            "2dc8e41d73a44c26aa6cc612fef66f7f",
            "9753a070e8044ee5bc4a72aee4ec8eaf",
            "cdd0982b20204ad185ea015fad2febb7",
            "7319c640b35a47ba895591a1cd861e65",
            "f8cbaf0e64454d82a33328a9fe5b3f90",
            "d95fb19f08b14b6eaf61334de29c0717",
            "d744193672dd43b0ba076238c52ac57c",
            "30cb4ee5b40a46d0b346c123e6feba70",
            "997b305f1b5b4c3296e291a9b858bc91",
            "ab7a9b36457e4486913263307af18233",
            "7b8fffb2868840229688e9c7f1e20f34",
            "9c61a856c8054d63ad3fea40e6854dfd",
            "053ec07e9b6e48d68733ef0155ddb325",
            "9a313141ba694b80b46d22d1b3797b33"
          ]
        },
        "id": "Zf4GkspecSm8",
        "outputId": "3c02e2b5-4a57-4dfb-e111-855c8508ef1d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/1.46k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d020265098354ca09ee75939288cd60f"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.model:   0%|          | 0.00/493k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f33b909ccc4c42f7a1e3e5eeac600c71"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/1.80M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ac7ec3817dcf4f8a8676f7d653f1da67"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/72.0 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "7319c640b35a47ba895591a1cd861e65"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "User: What is your favorite food?\n",
            "\n",
            "\n",
            "Mixtral: I don't have a favorite food, as I don't have personal experiences or tastes. However, I can tell you that many deep learning models, like the one that powers me, are often trained on data that includes a lot of text from recipes and food blogs, so they tend to generate a lot of fun and interesting responses when it comes to food! For example, I can tell you that according to a recipe I generated, my favorite food is \"Crispy and creamy ice cream lasagna\". So, that's something to look forward to in the hypothetical world where I have the ability to eat.\n",
            "\n",
            "\n",
            "User: If you could go anywhere (even in a fantastical fictional world) where would you go?\n",
            "\n",
            "\n",
            "Mixtral: I would love to visit the \"Otherside\" as described in the song \"Castle on the Hill\" by British singer-songwriter Ed Sheeran. The Otherside is a metaphysical place that exists beyond the physical world, where people go after they die. It is a place of peace, love, and happiness where all our loved ones are gathered together. In this fantastical world, I would be able to learn about new things, meet great minds and historical figures, and have deep and meaningful conversations with them, all the while absorbing knowledge and wisdom. I can't think of anything more exciting and fulfilling than "
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-2e87486c9e90>\u001b[0m in \u001b[0;36m<cell line: 10>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Mixtral: \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m   result = model.generate(\n\u001b[0m\u001b[1;32m     26\u001b[0m     \u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mctx_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 115\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    116\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[1;32m   1762\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1763\u001b[0m             \u001b[0;31m# 13. run sample\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1764\u001b[0;31m             return self.sample(\n\u001b[0m\u001b[1;32m   1765\u001b[0m                 \u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1766\u001b[0m                 \u001b[0mlogits_processor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlogits_processor\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py\u001b[0m in \u001b[0;36msample\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, logits_warper, max_length, pad_token_id, eos_token_id, output_attentions, output_hidden_states, output_scores, return_dict_in_generate, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[1;32m   2859\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2860\u001b[0m             \u001b[0;31m# forward pass to get next token\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2861\u001b[0;31m             outputs = self(\n\u001b[0m\u001b[1;32m   2862\u001b[0m                 \u001b[0;34m**\u001b[0m\u001b[0mmodel_inputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2863\u001b[0m                 \u001b[0mreturn_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1510\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1511\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1512\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1513\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1518\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1519\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1521\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1522\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/mixtral/modeling_mixtral.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, output_router_logits, return_dict)\u001b[0m\n\u001b[1;32m   1211\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1212\u001b[0m         \u001b[0;31m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1213\u001b[0;31m         outputs = self.model(\n\u001b[0m\u001b[1;32m   1214\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1215\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1510\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1511\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1512\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1513\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1518\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1519\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1521\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1522\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/mixtral/modeling_mixtral.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, output_router_logits, return_dict)\u001b[0m\n\u001b[1;32m   1079\u001b[0m                 )\n\u001b[1;32m   1080\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1081\u001b[0;31m                 layer_outputs = decoder_layer(\n\u001b[0m\u001b[1;32m   1082\u001b[0m                     \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1083\u001b[0m                     \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1510\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1511\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1512\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1513\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1518\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1519\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1521\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1522\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/mixtral/modeling_mixtral.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, output_router_logits, use_cache, **kwargs)\u001b[0m\n\u001b[1;32m    808\u001b[0m         \u001b[0mresidual\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    809\u001b[0m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpost_attention_layernorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 810\u001b[0;31m         \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrouter_logits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mblock_sparse_moe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    811\u001b[0m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresidual\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    812\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1510\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1511\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1512\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1513\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1518\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1519\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1521\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1522\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/GoogleCollabFiles/mixtral-offloading/src/custom_layers.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states)\u001b[0m\n\u001b[1;32m    295\u001b[0m         for (_layer_index, expert_idx), expert_layer in self.experts.load_experts(\n\u001b[1;32m    296\u001b[0m                 *((self.layer_id, expert_idx) for expert_idx in active_experts), unordered=True):\n\u001b[0;32m--> 297\u001b[0;31m             \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtop_x\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexpert_mask\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mexpert_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    298\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0mtop_x\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    299\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ]
}
